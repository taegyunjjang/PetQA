{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "raw_dir_path = \"/home/work/factchecking/PetQA/data/raw/\"\n",
    "interim_dir_path = \"/home/work/factchecking/PetQA/data/interim/\"\n",
    "file_list = [f for f in os.listdir(raw_dir_path) if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6개의 raw 파일 -> 하나의 파일로 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 게시글의 수: 63376\n"
     ]
    }
   ],
   "source": [
    "merged_data_path = os.path.join(interim_dir_path, \"merged_data.json\")\n",
    "merged_data = []\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(raw_dir_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    merged_data.extend(data)\n",
    "\n",
    "with open(merged_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"총 게시글의 수: {len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피쳐 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral data count: 1351\n",
      "null data count: 7\n",
      "duplicated data count: 193\n",
      "file_path: /home/work/factchecking/PetQA/data/interim/preprocessed_data.json\n",
      "Total data count: 61,825\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "extracted_data = []\n",
    "neutral_data_count = 0\n",
    "null_data_count = 0\n",
    "NULL_STR = \"본 게시물은 개인정보노출 또는 서비스 운영원칙에 위배된 내용이 포함되어 삭제되었습니다.\"\n",
    "\n",
    "URL_PATTERN = re.compile(r'https?://\\S+')\n",
    "def remove_urls(text: str) -> str:\n",
    "    return URL_PATTERN.sub('', text)\n",
    "\n",
    "for item in merged_data:\n",
    "    if item.get(\"label_str\") == \"neutral\":\n",
    "        neutral_data_count += 1\n",
    "        continue\n",
    "    if item.get(\"제목\") == NULL_STR:\n",
    "        null_data_count += 1\n",
    "        continue\n",
    "    \n",
    "    new_item = {\n",
    "        \"title\": remove_urls(item.get(\"제목\", \"\")),\n",
    "        \"content\": remove_urls(item.get(\"본문\", \"\")),\n",
    "        \"answers\": [],\n",
    "        \"question_date\": item.get(\"question_date\"),\n",
    "        \"animal_type\": item.get(\"label_str\"),\n",
    "        \"link\": item.get(\"link\")\n",
    "    }\n",
    "    \n",
    "    for a_id, answer in enumerate(item[\"answers\"]):\n",
    "        if answer.get(\"답변\") == \"\":\n",
    "            continue\n",
    "        \n",
    "        answer_type = \"expert\" if answer.get(\"expert_badge\") == \"수의사\" else \"nonexpert\"\n",
    "        new_answer = {\n",
    "            \"a_id\": a_id,\n",
    "            \"answer_type\": answer_type,\n",
    "            \"answer\": remove_urls(answer.get(\"답변\", \"\")),\n",
    "            \"selected\": answer.get(\"selected\"),\n",
    "            \"name\": answer.get(\"name\"),\n",
    "            \"badge\": answer.get(\"badge\"),\n",
    "            \"answer_date\": answer.get(\"answer_date\")\n",
    "        }\n",
    "        new_item[\"answers\"].append(new_answer)\n",
    "    \n",
    "    if new_item[\"answers\"]:\n",
    "        extracted_data.append(new_item)\n",
    "        \n",
    "seen_titles_contents = set()\n",
    "duplicated_data = 0\n",
    "q_id_count = 0\n",
    "unique_data = []\n",
    "\n",
    "for item in extracted_data:\n",
    "    identifier = (item['title'], item['content'])\n",
    "    if identifier in seen_titles_contents:\n",
    "        duplicated_data += 1\n",
    "    else:\n",
    "        seen_titles_contents.add(identifier)\n",
    "        new_item = {\"q_id\": q_id_count}\n",
    "        new_item.update(item)\n",
    "        unique_data.append(new_item)\n",
    "        q_id_count += 1\n",
    "\n",
    "output_path = os.path.join(interim_dir_path, \"preprocessed_data.json\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "print(f\"neutral data count: {neutral_data_count}\")\n",
    "print(f\"null data count: {null_data_count}\")\n",
    "print(f\"duplicated data count: {duplicated_data}\")\n",
    "print(f\"file_path: {output_path}\")\n",
    "print(f\"Total data count: {len(unique_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "URL_PATTERN = re.compile(r'https?://\\S+')\n",
    "\n",
    "def remove_urls(text: str) -> str:\n",
    "    return URL_PATTERN.sub('', text)\n",
    "\n",
    "def preprocess_and_filter(data):\n",
    "    cleaned_data = []\n",
    "    for item in data:\n",
    "        item[\"title\"] = remove_urls(item[\"title\"])\n",
    "        item[\"content\"] = remove_urls(item[\"content\"])\n",
    "        item[\"answer\"] = remove_urls(item[\"answer\"])\n",
    "        item[\"preprocessed_question\"] = remove_urls(item[\"preprocessed_question\"])\n",
    "        item[\"preprocessed_answer\"] = remove_urls(item[\"preprocessed_answer\"])\n",
    "        \n",
    "        if item[\"answer\"].strip() and item[\"preprocessed_answer\"].strip():\n",
    "            cleaned_data.append({\n",
    "                \"q_id\": item[\"q_id\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"content\": item[\"content\"],\n",
    "                \"answer\": item[\"answer\"],\n",
    "                \"a_id\": item[\"a_id\"],\n",
    "                \"answer_type\": item[\"answer_type\"],\n",
    "                \"question_date\": item[\"question_date\"],\n",
    "                \"animal_type\": item[\"animal_type\"],\n",
    "                \"preprocessed_question\": item[\"preprocessed_question\"],\n",
    "                \"preprocessed_answer\": item[\"preprocessed_answer\"],\n",
    "            })\n",
    "    return cleaned_data\n",
    "\n",
    "train_path = \"/home/work/factchecking/PetQA/data/processed/train.json\"\n",
    "validation_path = \"/home/work/factchecking/PetQA/data/processed/validation.json\"\n",
    "test_path = \"/home/work/factchecking/PetQA/data/processed/test.json\"\n",
    "\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(validation_path, 'r', encoding='utf-8') as f:\n",
    "    validation_data = json.load(f)\n",
    "\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "new_train_data = preprocess_and_filter(train_data)\n",
    "new_validation_data = preprocess_and_filter(validation_data)\n",
    "new_test_data = preprocess_and_filter(test_data)\n",
    "\n",
    "with open(train_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(validation_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_validation_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(test_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(new_test_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 수: 61825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 등급별 일반인 분포를 확인 목적\n",
    "# merged_data -> extracted_data_w_badge.json\n",
    "import json\n",
    "extracted_data = []\n",
    "NULL_STR = \"본 게시물은 개인정보노출 또는 서비스 운영원칙에 위배된 내용이 포함되어 삭제되었습니다.\"\n",
    "\n",
    "with open(\"/home/work/factchecking/PetQA/data/interim/merged_data.json\", 'r', encoding='utf-8') as f:\n",
    "    merged_data = json.load(f)\n",
    "\n",
    "\n",
    "        \n",
    "q_id_count = 0\n",
    "for item in merged_data:\n",
    "    if item.get(\"label_str\") == \"neutral\":\n",
    "        continue\n",
    "    if item.get(\"제목\") == NULL_STR:\n",
    "        continue\n",
    "    \n",
    "    new_item = {\n",
    "        \"q_id\": q_id_count,\n",
    "        \"title\": item.get(\"제목\"),\n",
    "        \"content\": item.get(\"본문\", \"\"),\n",
    "        \"answers\": [],\n",
    "        \"question_date\": item.get(\"question_date\"),\n",
    "        \"animal_type\": item.get(\"label_str\"),\n",
    "        \"link\": item.get(\"link\")\n",
    "    }\n",
    "    q_id_count += 1\n",
    "    \n",
    "    for a_id, answer in enumerate(item[\"answers\"]):\n",
    "        if answer.get(\"답변\") == \"\":\n",
    "            continue\n",
    "        \n",
    "        answer_type = \"expert\" if answer.get(\"expert_badge\") == \"수의사\" else \"nonexpert\"\n",
    "        new_answer = {\n",
    "            \"a_id\": a_id,\n",
    "            \"answer_type\": answer_type,\n",
    "            \"answer\": answer.get(\"답변\"),\n",
    "            \"badge\": answer.get(\"badge\"),\n",
    "            \"selected\": answer.get(\"selected\"),\n",
    "            \"answer_date\": answer.get(\"answer_date\")\n",
    "        }\n",
    "        new_item[\"answers\"].append(new_answer)\n",
    "    extracted_data.append(new_item)\n",
    "    \n",
    "seen_titles_contents = set()\n",
    "duplicated_data = 0\n",
    "unique_data = []\n",
    "\n",
    "for item in extracted_data:\n",
    "    identifier = (item['title'], item['content'])\n",
    "    if identifier in seen_titles_contents:\n",
    "        duplicated_data += 1\n",
    "    else:\n",
    "        seen_titles_contents.add(identifier)\n",
    "        new_item = {\"q_id\": q_id_count}\n",
    "        new_item.update(item)\n",
    "        unique_data.append(new_item)\n",
    "\n",
    "print(f\"샘플 수: {len(unique_data)}\")\n",
    "# with open(\"./extracted_data_w_badge.json\", 'w', encoding='utf-8') as f:\n",
    "#     json.dump(unique_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home/work/factchecking/PetQA/data/interim/cleaned_data.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "print(f\"전처리 후 # QA pairs: {len(cleaned_data):,}\")\n",
    "\n",
    "file_path = \"/home/work/factchecking/PetQA/src/preprocessing/extracted_data_w_badge.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "selected_all_answer_type_q_ids = []\n",
    "for item in data:\n",
    "    selected_expert_flag = False\n",
    "    selected_nonexpert_flag = False\n",
    "    for answer in item[\"answers\"]:\n",
    "        if answer[\"answer_type\"] == \"expert\" and answer[\"selected\"]:\n",
    "            selected_expert_flag = True\n",
    "        if answer[\"answer_type\"] == \"nonexpert\" and answer[\"selected\"]:\n",
    "            selected_nonexpert_flag = True\n",
    "    if selected_expert_flag and selected_nonexpert_flag:\n",
    "        selected_all_answer_type_q_ids.append(item[\"q_id\"])\n",
    "print(len(selected_all_answer_type_q_ids))\n",
    "\n",
    "q_ids = []\n",
    "for item in cleaned_data:\n",
    "    if item[\"q_id\"] in selected_all_answer_type_q_ids:\n",
    "        q_ids.append(item[\"q_id\"])\n",
    "print(len(q_ids))\n",
    "\n",
    "import os\n",
    "processed_dir = \"/home/work/factchecking/PetQA/data/processed\"\n",
    "train_file = os.path.join(processed_dir, \"train.json\")\n",
    "validation_file = os.path.join(processed_dir, \"validation.json\")\n",
    "test_file = os.path.join(processed_dir, \"test.json\")\n",
    "\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "    print(len(train_data))\n",
    "with open(validation_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    validation_data = json.load(f)\n",
    "    print(len(validation_data))\n",
    "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "    print(len(test_data))\n",
    "    \n",
    "new_train_data = []\n",
    "for item in train_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_train_data.append(item)\n",
    "\n",
    "new_validation_data = []\n",
    "for item in validation_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_validation_data.append(item)\n",
    "\n",
    "new_test_data = []\n",
    "for item in test_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_test_data.append(item)\n",
    "\n",
    "print(len(new_train_data))\n",
    "print(len(new_validation_data))\n",
    "print(len(new_test_data))\n",
    "\n",
    "new_train_path = os.path.join(processed_dir, \"train.json\")\n",
    "with open(new_train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "new_validation_path = os.path.join(processed_dir, \"validation.json\")\n",
    "with open(new_validation_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_validation_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "new_test_path = os.path.join(processed_dir, \"test.json\")\n",
    "with open(new_test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_test_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 55393\n",
      "--------------------------------------------------\n",
      "원본 데이터 카테고리 분포:\n",
      "cat-expert: 2289개 (4.1%)\n",
      "dog-expert: 11535개 (20.8%)\n",
      "cat-nonexpert: 16402개 (29.6%)\n",
      "dog-nonexpert: 25167개 (45.4%)\n",
      "--------------------------------------------------\n",
      "분할 결과:\n",
      "Train: 35393개\n",
      "Validation: 10000개\n",
      "Test: 10000개\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "cleaned_data_path = \"/home/work/factchecking/PetQA/data/interim/cleaned_data.json\"\n",
    "output_dir = \"/home/work/factchecking/PetQA/data/processed\"\n",
    "\n",
    "with open(cleaned_data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "total_count = len(data)\n",
    "print(f\"전체 데이터 크기: {total_count}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "stratify_labels = []\n",
    "for item in data:\n",
    "    category = f\"{item['animal_type']}-{item['answer_type']}\"\n",
    "    stratify_labels.append(category)\n",
    "    \n",
    "category_counts = Counter(stratify_labels)\n",
    "print(\"원본 데이터 카테고리 분포:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / total_count) * 100\n",
    "    print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    data, \n",
    "    stratify_labels,\n",
    "    test_size=20000,  # val + test\n",
    "    train_size=total_count - 20000,  # train\n",
    "    stratify=stratify_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # temp의 절반씩\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"분할 결과:\")\n",
    "print(f\"Train: {len(train_data)}개\")\n",
    "print(f\"Validation: {len(val_data)}개\") \n",
    "print(f\"Test: {len(test_data)}개\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 카테고리 분포:\n",
      "cat-expert: 1463개 (4.1%)\n",
      "cat-nonexpert: 10480개 (29.6%)\n",
      "dog-expert: 7370개 (20.8%)\n",
      "dog-nonexpert: 16080개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Validation 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2082개 (20.8%)\n",
      "dog-nonexpert: 4544개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Test 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2083개 (20.8%)\n",
      "dog-nonexpert: 4543개 (45.4%)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(data_labels, dataset_name):\n",
    "    category_counts = Counter(data_labels)\n",
    "    total = len(data_labels)\n",
    "    print(f\"{dataset_name} 카테고리 분포:\")\n",
    "    for category, count in sorted(category_counts.items()):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print_distribution(train_labels, \"Train\")\n",
    "print_distribution(val_labels, \"Validation\") \n",
    "print_distribution(test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json 저장 완료: /home/work/factchecking/PetQA/data/processed/train.json\n",
      "val.json 저장 완료: /home/work/factchecking/PetQA/data/processed/val.json\n",
      "test.json 저장 완료: /home/work/factchecking/PetQA/data/processed/test.json\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'train': train_data,\n",
    "    'validation': val_data, \n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "for split_name, split_data in datasets.items():\n",
    "    output_path = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(split_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{split_name}.json 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category             Original   Train      Val        Test      \n",
      "-----------------------------------------------------------------\n",
      "cat-expert           0.041     0.041     0.041     0.041\n",
      "cat-nonexpert        0.296     0.296     0.296     0.296\n",
      "dog-expert           0.208     0.208     0.208     0.208\n",
      "dog-nonexpert        0.454     0.454     0.454     0.454\n"
     ]
    }
   ],
   "source": [
    "# 분포 확인\n",
    "all_data = train_data + val_data + test_data\n",
    "    \n",
    "def get_category_distribution(data):\n",
    "    categories = [f\"{item['animal_type']}-{item['answer_type']}\" for item in data]\n",
    "    counter = Counter(categories)\n",
    "    total = len(data)\n",
    "    return {cat: count/total for cat, count in counter.items()}\n",
    "\n",
    "original_dist = get_category_distribution(all_data)\n",
    "train_dist = get_category_distribution(train_data)\n",
    "val_dist = get_category_distribution(val_data)\n",
    "test_dist = get_category_distribution(test_data)\n",
    "\n",
    "print(f\"{'Category':<20} {'Original':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for category in sorted(original_dist.keys()):\n",
    "    print(f\"{category:<20} {original_dist[category]:.3f}     {train_dist[category]:.3f}     {val_dist[category]:.3f}     {test_dist[category]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
