{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 게시글 수: 12,645\n",
      "dict_keys(['제목', '본문', 'question_video', 'tag_list', 'link', 'question_photo', 'question_date', 'answers', 'html_path'])\n",
      "총 이미지 수: 20,153\n",
      "818092f7-27ac-4c52-b69a-ba9d722a21c0.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "raw_data_path = \"/home/work/factchecking/PetQA/data/raw/filtered_medical_consultation_expert.json\"\n",
    "img_folder = Path(\"/home/work/factchecking/PetQA/data/processed/question_images\")\n",
    "\n",
    "with open(raw_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(f\"총 게시글 수: {len(data):,}\")\n",
    "print(data[0].keys())\n",
    "\n",
    "img_list = os.listdir(img_folder)\n",
    "print(f\"총 이미지 수: {len(img_list):,}\")\n",
    "print(img_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플별 이미지 개수 분포:\n",
      "1개 이미지: 7705개 샘플\n",
      "2개 이미지: 3261개 샘플\n",
      "3개 이미지: 1069개 샘플\n",
      "4개 이미지: 357개 샘플\n",
      "5개 이미지: 122개 샘플\n",
      "6개 이미지: 62개 샘플\n",
      "7개 이미지: 32개 샘플\n",
      "8개 이미지: 15개 샘플\n",
      "9개 이미지: 6개 샘플\n",
      "10개 이미지: 3개 샘플\n",
      "11개 이미지: 1개 샘플\n",
      "12개 이미지: 3개 샘플\n",
      "13개 이미지: 2개 샘플\n",
      "16개 이미지: 2개 샘플\n",
      "18개 이미지: 1개 샘플\n",
      "23개 이미지: 2개 샘플\n",
      "24개 이미지: 1개 샘플\n",
      "42개 이미지: 1개 샘플\n",
      "총 샘플 수: 12645\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "img_counts = []\n",
    "for item in data:\n",
    "    img_counts.append(len(item[\"question_photo\"]))\n",
    "\n",
    "count_distribution = Counter(img_counts)\n",
    "print(\"샘플별 이미지 개수 분포:\")\n",
    "total = 0\n",
    "for num_imgs, freq in sorted(count_distribution.items()):\n",
    "    print(f\"{num_imgs}개 이미지: {freq}개 샘플\")\n",
    "    total += freq\n",
    "print(f\"총 샘플 수: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문당 평균 이미지 수: 1.62\n",
      "질문당 최소 이미지 수: 1\n",
      "질문당 최대 이미지 수: 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 질문당 이미지 개수 통계\n",
    "img_counts_array = np.array(img_counts)\n",
    "\n",
    "avg_imgs = np.mean(img_counts_array)\n",
    "min_imgs = np.min(img_counts_array)\n",
    "max_imgs = np.max(img_counts_array)\n",
    "\n",
    "print(f\"질문당 평균 이미지 수: {avg_imgs:.2f}\")\n",
    "print(f\"질문당 최소 이미지 수: {min_imgs}\")\n",
    "print(f\"질문당 최대 이미지 수: {max_imgs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 수: 20153\n",
      "너비 평균: 722.6, 표준편차: 79.8, 최소~최대: 19~750\n",
      "높이 평균: 874.1, 표준편차: 307.7, 최소~최대: 19~4496\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "widths, heights = [], []\n",
    "\n",
    "for item in data:\n",
    "    # question_photos = ast.literal_eval(item[\"question_photo\"])\n",
    "    for img_name in item[\"question_photo\"]:\n",
    "        img_path = img_folder / f\"{img_name}.jpg\"\n",
    "        if img_path.exists():\n",
    "            with Image.open(img_path) as img:\n",
    "                w, h = img.size\n",
    "                widths.append(w)\n",
    "                heights.append(h)\n",
    "\n",
    "# 통계 출력\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "print(f\"이미지 수: {len(widths)}\")\n",
    "print(f\"너비 평균: {np.mean(widths):.1f}, 표준편차: {np.std(widths):.1f}, 최소~최대: {np.min(widths)}~{np.max(widths)}\")\n",
    "print(f\"높이 평균: {np.mean(heights):.1f}, 표준편차: {np.std(heights):.1f}, 최소~최대: {np.min(heights)}~{np.max(heights)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "채택된 수의사 답변 수: 11,263 (89.07%)\n",
      "미채택 수의사 답변 수: 1,384 (10.95%)\n"
     ]
    }
   ],
   "source": [
    "# 수의사 샘플 필터링\n",
    "selected_expert_data = []\n",
    "non_selected_expert_cnt = 0\n",
    "for item in data:\n",
    "    new_item = {\n",
    "        \"image\": item[\"question_photo\"][0],\n",
    "        \"title\": item.get(\"제목\", \"\"),\n",
    "        \"content\": item.get(\"본문\", \"\"),\n",
    "        \"answers\": [],\n",
    "    }\n",
    "    for a_id, answer in enumerate(item[\"answers\"]):\n",
    "        if answer.get(\"expert_badge\") != \"수의사\":\n",
    "            continue\n",
    "        if not answer.get(\"selected\"):\n",
    "            non_selected_expert_cnt += 1\n",
    "            continue\n",
    "        \n",
    "        new_answer = {\n",
    "            \"a_id\": a_id,\n",
    "            \"answer\": answer[\"답변\"],\n",
    "        }\n",
    "        new_item[\"answers\"].append(new_answer)\n",
    "    \n",
    "    if new_item[\"answers\"]:\n",
    "        selected_expert_data.append(new_item)\n",
    "\n",
    "print(f\"채택된 수의사 답변 수: {len(selected_expert_data):,} ({len(selected_expert_data) / len(data):.2%})\")\n",
    "print(f\"미채택 수의사 답변 수: {non_selected_expert_cnt:,} ({non_selected_expert_cnt / len(data):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여러 수의사 답변 수: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "multi_expert_cnt = 0\n",
    "for item in selected_expert_data:\n",
    "    if len(item[\"answers\"]) > 1:\n",
    "        multi_expert_cnt += 1\n",
    "\n",
    "print(f\"여러 수의사 답변 수: {multi_expert_cnt:,} ({multi_expert_cnt / len(selected_expert_data):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(question) < 20인 샘플 수: 29\n",
      "삭제된 게시글 수: 196\n",
      "개, 고양이가 아닌 샘플 수: 0\n",
      "11,038\n"
     ]
    }
   ],
   "source": [
    "neutral_data_count = 0\n",
    "null_data_count = 0\n",
    "too_short_question_count = 0\n",
    "too_short_answer_count = 0\n",
    "deleted_cnt = 0  # 링크 접속이 안되는 http로 시작하는 이미지 제거\n",
    "\n",
    "new_data = []\n",
    "for id, item in enumerate(selected_expert_data):\n",
    "    if len(item[\"title\"] + \" \" + item[\"content\"]) < 20:\n",
    "        too_short_question_count += 1\n",
    "        continue\n",
    "    \n",
    "    if len(item[\"answers\"][0][\"answer\"]) < 20:\n",
    "        too_short_answer_count += 1\n",
    "        continue\n",
    "    \n",
    "    if item[\"image\"].startswith(\"http\"):\n",
    "        deleted_cnt += 1\n",
    "        continue\n",
    "    \n",
    "    if item[\"answers\"][0][\"answer\"] == \"\":\n",
    "        print(id)  # 빈 답변 확인인\n",
    "        continue\n",
    "    \n",
    "    # if item.get(\"label_str\") == \"neutral\":\n",
    "    #     neutral_data_count += 1\n",
    "    #     continue\n",
    "    \n",
    "    new_data.append({\n",
    "        \"id\": id,\n",
    "        \"image\": item[\"image\"],\n",
    "        \"title\": item[\"title\"],\n",
    "        \"content\": item[\"content\"],\n",
    "        \"answer\": item[\"answers\"][0][\"answer\"],\n",
    "    })\n",
    "\n",
    "print(f\"len(question) < 20인 샘플 수: {too_short_question_count:,}\")\n",
    "print(f\"len(answer) < 20인 샘플 수: {too_short_answer_count:,}\")\n",
    "print(f\"삭제된 게시글 수: {deleted_cnt:,}\")\n",
    "print(f\"개, 고양이가 아닌 샘플 수: {neutral_data_count:,}\")\n",
    "print(f\"{len(new_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 샘플 수: 0\n",
      "중복 제거 후 샘플 수: 11,038\n"
     ]
    }
   ],
   "source": [
    "# 중복(동일 게시글) 제거\n",
    "seen_titles_contents = set()\n",
    "deduplicated_data = []\n",
    "\n",
    "for item in new_data:\n",
    "    identifier = (item['title'], item['content'], item['image'])\n",
    "    if identifier not in seen_titles_contents:\n",
    "        seen_titles_contents.add(identifier)\n",
    "        deduplicated_data.append(item)\n",
    "\n",
    "print(f\"중복된 샘플 수: {len(new_data) - len(deduplicated_data)}\")\n",
    "print(f\"중복 제거 후 샘플 수: {len(deduplicated_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "짧은 답변 수: 5\n",
      "최종 샘플 수: 11,033\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "short_answer_cnt = 0\n",
    "cleaned_data = []\n",
    "URL_PATTERN = re.compile(r'https?://\\S+')\n",
    "def remove_urls(text: str) -> str:\n",
    "    return URL_PATTERN.sub('', text)\n",
    "\n",
    "def remove_common_greetings(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # 유니코드 특수문자(제로폭 공백 등) 제거\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\u2060\\ufeff]', '', text)\n",
    "    patterns = [\n",
    "        r'^안녕하세요.*?입니다\\.?\\s*',\n",
    "        r'\\s*감사합니다\\.?\\s*$',\n",
    "        r'\\s*고맙습니다\\.?\\s*$',\n",
    "        r'\\s*안녕히\\s*계세요\\.?\\s*$',\n",
    "        r'\\s*안녕히\\s*가세요\\.?\\s*$',\n",
    "        r'\\s*수고하세요\\.?\\s*$',\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE | re.UNICODE).strip()\n",
    "    return text\n",
    "\n",
    "for item in deduplicated_data:\n",
    "    answer = remove_urls(remove_common_greetings(item[\"answer\"]))\n",
    "    if len(answer) < 20:\n",
    "        short_answer_cnt += 1\n",
    "        continue\n",
    "    cleaned_data.append({\n",
    "        \"id\": item[\"id\"],\n",
    "        \"image\": item[\"image\"],\n",
    "        \"title\": remove_urls(remove_common_greetings(item[\"title\"])),\n",
    "        \"content\": remove_urls(remove_common_greetings(item[\"content\"])),\n",
    "        \"answer\": answer\n",
    "    })\n",
    "\n",
    "output_path = \"/home/work/factchecking/PetQA/data/interim/preprocessed_multimodal_data.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"짧은 답변 수: {short_answer_cnt:,}\")\n",
    "print(f\"최종 샘플 수: {len(cleaned_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "짧은 답변 수: 5\n",
      "[1058, 2953, 6310, 6798, 9250]\n"
     ]
    }
   ],
   "source": [
    "short_answer = []\n",
    "for item in cleaned_data:\n",
    "    if len(item[\"answer\"]) < 20:\n",
    "        short_answer.append(item[\"id\"])\n",
    "\n",
    "print(f\"짧은 답변 수: {len(short_answer):,}\")\n",
    "print(short_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = \"/home/work/factchecking/PetQA/data/interim/cleaned_data.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    cleaned_data = json.load(f)\n",
    "print(f\"전처리 후 # QA pairs: {len(cleaned_data):,}\")\n",
    "\n",
    "file_path = \"/home/work/factchecking/PetQA/src/preprocessing/extracted_data_w_badge.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))\n",
    "print(data[0])\n",
    "\n",
    "selected_all_answer_type_q_ids = []\n",
    "for item in data:\n",
    "    selected_expert_flag = False\n",
    "    selected_nonexpert_flag = False\n",
    "    for answer in item[\"answers\"]:\n",
    "        if answer[\"answer_type\"] == \"expert\" and answer[\"selected\"]:\n",
    "            selected_expert_flag = True\n",
    "        if answer[\"answer_type\"] == \"nonexpert\" and answer[\"selected\"]:\n",
    "            selected_nonexpert_flag = True\n",
    "    if selected_expert_flag and selected_nonexpert_flag:\n",
    "        selected_all_answer_type_q_ids.append(item[\"q_id\"])\n",
    "print(len(selected_all_answer_type_q_ids))\n",
    "\n",
    "q_ids = []\n",
    "for item in cleaned_data:\n",
    "    if item[\"q_id\"] in selected_all_answer_type_q_ids:\n",
    "        q_ids.append(item[\"q_id\"])\n",
    "print(len(q_ids))\n",
    "\n",
    "import os\n",
    "processed_dir = \"/home/work/factchecking/PetQA/data/processed\"\n",
    "train_file = os.path.join(processed_dir, \"train.json\")\n",
    "validation_file = os.path.join(processed_dir, \"validation.json\")\n",
    "test_file = os.path.join(processed_dir, \"test.json\")\n",
    "\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "    print(len(train_data))\n",
    "with open(validation_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    validation_data = json.load(f)\n",
    "    print(len(validation_data))\n",
    "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "    print(len(test_data))\n",
    "    \n",
    "new_train_data = []\n",
    "for item in train_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_train_data.append(item)\n",
    "\n",
    "new_validation_data = []\n",
    "for item in validation_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_validation_data.append(item)\n",
    "\n",
    "new_test_data = []\n",
    "for item in test_data:\n",
    "    if item[\"q_id\"] not in q_ids:\n",
    "        new_test_data.append(item)\n",
    "\n",
    "print(len(new_train_data))\n",
    "print(len(new_validation_data))\n",
    "print(len(new_test_data))\n",
    "\n",
    "new_train_path = os.path.join(processed_dir, \"train.json\")\n",
    "with open(new_train_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "new_validation_path = os.path.join(processed_dir, \"validation.json\")\n",
    "with open(new_validation_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_validation_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "new_test_path = os.path.join(processed_dir, \"test.json\")\n",
    "with open(new_test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(new_test_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 55393\n",
      "--------------------------------------------------\n",
      "원본 데이터 카테고리 분포:\n",
      "cat-expert: 2289개 (4.1%)\n",
      "dog-expert: 11535개 (20.8%)\n",
      "cat-nonexpert: 16402개 (29.6%)\n",
      "dog-nonexpert: 25167개 (45.4%)\n",
      "--------------------------------------------------\n",
      "분할 결과:\n",
      "Train: 35393개\n",
      "Validation: 10000개\n",
      "Test: 10000개\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "cleaned_data_path = \"/home/work/factchecking/PetQA/data/interim/cleaned_data.json\"\n",
    "output_dir = \"/home/work/factchecking/PetQA/data/processed\"\n",
    "\n",
    "with open(cleaned_data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "total_count = len(data)\n",
    "print(f\"전체 데이터 크기: {total_count}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "stratify_labels = []\n",
    "for item in data:\n",
    "    category = f\"{item['animal_type']}-{item['answer_type']}\"\n",
    "    stratify_labels.append(category)\n",
    "    \n",
    "category_counts = Counter(stratify_labels)\n",
    "print(\"원본 데이터 카테고리 분포:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / total_count) * 100\n",
    "    print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    data, \n",
    "    stratify_labels,\n",
    "    test_size=20000,  # val + test\n",
    "    train_size=total_count - 20000,  # train\n",
    "    stratify=stratify_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # temp의 절반씩\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"분할 결과:\")\n",
    "print(f\"Train: {len(train_data)}개\")\n",
    "print(f\"Validation: {len(val_data)}개\") \n",
    "print(f\"Test: {len(test_data)}개\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 카테고리 분포:\n",
      "cat-expert: 1463개 (4.1%)\n",
      "cat-nonexpert: 10480개 (29.6%)\n",
      "dog-expert: 7370개 (20.8%)\n",
      "dog-nonexpert: 16080개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Validation 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2082개 (20.8%)\n",
      "dog-nonexpert: 4544개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Test 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2083개 (20.8%)\n",
      "dog-nonexpert: 4543개 (45.4%)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(data_labels, dataset_name):\n",
    "    category_counts = Counter(data_labels)\n",
    "    total = len(data_labels)\n",
    "    print(f\"{dataset_name} 카테고리 분포:\")\n",
    "    for category, count in sorted(category_counts.items()):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print_distribution(train_labels, \"Train\")\n",
    "print_distribution(val_labels, \"Validation\") \n",
    "print_distribution(test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json 저장 완료: /home/work/factchecking/PetQA/data/processed/train.json\n",
      "val.json 저장 완료: /home/work/factchecking/PetQA/data/processed/val.json\n",
      "test.json 저장 완료: /home/work/factchecking/PetQA/data/processed/test.json\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'train': train_data,\n",
    "    'validation': val_data, \n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "for split_name, split_data in datasets.items():\n",
    "    output_path = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(split_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{split_name}.json 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category             Original   Train      Val        Test      \n",
      "-----------------------------------------------------------------\n",
      "cat-expert           0.041     0.041     0.041     0.041\n",
      "cat-nonexpert        0.296     0.296     0.296     0.296\n",
      "dog-expert           0.208     0.208     0.208     0.208\n",
      "dog-nonexpert        0.454     0.454     0.454     0.454\n"
     ]
    }
   ],
   "source": [
    "# 분포 확인\n",
    "all_data = train_data + val_data + test_data\n",
    "    \n",
    "def get_category_distribution(data):\n",
    "    categories = [f\"{item['animal_type']}-{item['answer_type']}\" for item in data]\n",
    "    counter = Counter(categories)\n",
    "    total = len(data)\n",
    "    return {cat: count/total for cat, count in counter.items()}\n",
    "\n",
    "original_dist = get_category_distribution(all_data)\n",
    "train_dist = get_category_distribution(train_data)\n",
    "val_dist = get_category_distribution(val_data)\n",
    "test_dist = get_category_distribution(test_data)\n",
    "\n",
    "print(f\"{'Category':<20} {'Original':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for category in sorted(original_dist.keys()):\n",
    "    print(f\"{category:<20} {original_dist[category]:.3f}     {train_dist[category]:.3f}     {val_dist[category]:.3f}     {test_dist[category]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
