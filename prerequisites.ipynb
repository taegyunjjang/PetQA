{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "raw_dir_path = \"/home/work/factchecking/PetQA/data/raw/\"\n",
    "interim_dir_path = \"/home/work/factchecking/PetQA/data/interim/\"\n",
    "file_list = [f for f in os.listdir(raw_dir_path) if f.endswith('.json')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6개의 raw 파일 -> 하나의 파일로 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 병합 완료: /home/work/factchecking/PetQA/data/interim/merged_data.json\n",
      "총 샘플의 수: 63376\n"
     ]
    }
   ],
   "source": [
    "merged_data_path = os.path.join(interim_dir_path, \"merged_data.json\")\n",
    "merged_data = []\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(raw_dir_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    merged_data.extend(data)\n",
    "\n",
    "with open(merged_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"파일 병합 완료: {merged_data_path}\")\n",
    "print(f\"총 샘플의 수: {len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피쳐 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추출 완료: /home/work/factchecking/PetQA/data/interim/extracted_data.json\n",
      "중립 데이터 수: 1351\n",
      "삭제된 데이터 수: 7\n",
      "추출된 샘플 수: 62018\n"
     ]
    }
   ],
   "source": [
    "extracted_data_path = os.path.join(interim_dir_path, \"extracted_data.json\")\n",
    "extracted_data = []\n",
    "neutral_data_count = 0\n",
    "null_data_count = 0\n",
    "NULL_STR = \"본 게시물은 개인정보노출 또는 서비스 운영원칙에 위배된 내용이 포함되어 삭제되었습니다.\"\n",
    "\n",
    "with open(merged_data_path, 'r', encoding='utf-8') as f:\n",
    "    merged_data = json.load(f)\n",
    "\n",
    "for item in merged_data:\n",
    "    if item.get(\"label_str\") == \"neutral\":\n",
    "        neutral_data_count += 1\n",
    "        continue\n",
    "    if item.get(\"제목\") == NULL_STR:\n",
    "        null_data_count += 1\n",
    "        continue\n",
    "    \n",
    "    new_item = {\n",
    "        \"title\": item.get(\"제목\"),\n",
    "        \"content\": item.get(\"본문\", \"\"),\n",
    "        \"answers\": [],\n",
    "        \"question_date\": item.get(\"question_date\"),\n",
    "        \"animal_type\": item.get(\"label_str\"),\n",
    "        \"link\": item.get(\"link\")\n",
    "    }\n",
    "    \n",
    "    for a_id, answer in enumerate(item[\"answers\"]):\n",
    "        answer_type = \"expert\" if answer.get(\"expert_badge\") == \"수의사\" else \"nonexpert\"\n",
    "        new_answer = {\n",
    "            \"a_id\": a_id,\n",
    "            \"answer_type\": answer_type,\n",
    "            \"answer\": answer.get(\"답변\"),\n",
    "            \"selected\": answer.get(\"selected\"),\n",
    "            \"answer_date\": answer.get(\"answer_date\")\n",
    "        }\n",
    "        new_item[\"answers\"].append(new_answer)\n",
    "    extracted_data.append(new_item)\n",
    "\n",
    "with open(extracted_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(extracted_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "print(f\"추출 완료: {extracted_data_path}\")\n",
    "print(f\"중립 데이터 수: {neutral_data_count}\")\n",
    "print(f\"삭제된 데이터 수: {null_data_count}\")\n",
    "print(f\"추출된 샘플 수: {len(extracted_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "중복 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 데이터: 193개\n",
      "고유한 샘플 수: 61825개\n"
     ]
    }
   ],
   "source": [
    "unique_data_path = os.path.join(interim_dir_path, \"unique_data.json\")\n",
    "with open(extracted_data_path, 'r', encoding='utf-8') as f:\n",
    "\tdata = json.load(f)\n",
    "\n",
    "seen_titles_contents = set()\n",
    "duplicated_data = 0\n",
    "unique_data = []\n",
    "q_id_count = 0\n",
    "\n",
    "for item in data:\n",
    "    identifier = (item['title'], item['content'])\n",
    "    if identifier in seen_titles_contents:\n",
    "        duplicated_data += 1\n",
    "    else:\n",
    "        seen_titles_contents.add(identifier)\n",
    "        new_item = {\"q_id\": q_id_count}\n",
    "        new_item.update(item)\n",
    "        unique_data.append(new_item)\n",
    "        q_id_count += 1\n",
    "        \n",
    "with open(unique_data_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"중복된 데이터: {duplicated_data}개\")\n",
    "print(f\"고유한 샘플 수: {len(unique_data)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터 크기: 55393\n",
      "--------------------------------------------------\n",
      "원본 데이터 카테고리 분포:\n",
      "cat-expert: 2289개 (4.1%)\n",
      "dog-expert: 11535개 (20.8%)\n",
      "cat-nonexpert: 16402개 (29.6%)\n",
      "dog-nonexpert: 25167개 (45.4%)\n",
      "--------------------------------------------------\n",
      "분할 결과:\n",
      "Train: 35393개\n",
      "Validation: 10000개\n",
      "Test: 10000개\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "cleaned_data_path = \"/home/work/factchecking/PetQA/data/interim/cleaned_data.json\"\n",
    "output_dir = \"/home/work/factchecking/PetQA/data/processed\"\n",
    "\n",
    "with open(cleaned_data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "total_count = len(data)\n",
    "print(f\"전체 데이터 크기: {total_count}\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "stratify_labels = []\n",
    "for item in data:\n",
    "    category = f\"{item['animal_type']}-{item['answer_type']}\"\n",
    "    stratify_labels.append(category)\n",
    "    \n",
    "category_counts = Counter(stratify_labels)\n",
    "print(\"원본 데이터 카테고리 분포:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / total_count) * 100\n",
    "    print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(\n",
    "    data, \n",
    "    stratify_labels,\n",
    "    test_size=20000,  # val + test\n",
    "    train_size=total_count - 20000,  # train\n",
    "    stratify=stratify_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(\n",
    "    temp_data,\n",
    "    temp_labels,\n",
    "    test_size=0.5,  # temp의 절반씩\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"분할 결과:\")\n",
    "print(f\"Train: {len(train_data)}개\")\n",
    "print(f\"Validation: {len(val_data)}개\") \n",
    "print(f\"Test: {len(test_data)}개\")\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 카테고리 분포:\n",
      "cat-expert: 1463개 (4.1%)\n",
      "cat-nonexpert: 10480개 (29.6%)\n",
      "dog-expert: 7370개 (20.8%)\n",
      "dog-nonexpert: 16080개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Validation 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2082개 (20.8%)\n",
      "dog-nonexpert: 4544개 (45.4%)\n",
      "--------------------------------------------------\n",
      "Test 카테고리 분포:\n",
      "cat-expert: 413개 (4.1%)\n",
      "cat-nonexpert: 2961개 (29.6%)\n",
      "dog-expert: 2083개 (20.8%)\n",
      "dog-nonexpert: 4543개 (45.4%)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_distribution(data_labels, dataset_name):\n",
    "    category_counts = Counter(data_labels)\n",
    "    total = len(data_labels)\n",
    "    print(f\"{dataset_name} 카테고리 분포:\")\n",
    "    for category, count in sorted(category_counts.items()):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"{category}: {count}개 ({percentage:.1f}%)\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "print_distribution(train_labels, \"Train\")\n",
    "print_distribution(val_labels, \"Validation\") \n",
    "print_distribution(test_labels, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.json 저장 완료: /home/work/factchecking/PetQA/data/processed/train.json\n",
      "val.json 저장 완료: /home/work/factchecking/PetQA/data/processed/val.json\n",
      "test.json 저장 완료: /home/work/factchecking/PetQA/data/processed/test.json\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'train': train_data,\n",
    "    'validation': val_data, \n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "for split_name, split_data in datasets.items():\n",
    "    output_path = os.path.join(output_dir, f\"{split_name}.json\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(split_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{split_name}.json 저장 완료: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category             Original   Train      Val        Test      \n",
      "-----------------------------------------------------------------\n",
      "cat-expert           0.041     0.041     0.041     0.041\n",
      "cat-nonexpert        0.296     0.296     0.296     0.296\n",
      "dog-expert           0.208     0.208     0.208     0.208\n",
      "dog-nonexpert        0.454     0.454     0.454     0.454\n"
     ]
    }
   ],
   "source": [
    "# 분포 확인\n",
    "all_data = train_data + val_data + test_data\n",
    "    \n",
    "def get_category_distribution(data):\n",
    "    categories = [f\"{item['animal_type']}-{item['answer_type']}\" for item in data]\n",
    "    counter = Counter(categories)\n",
    "    total = len(data)\n",
    "    return {cat: count/total for cat, count in counter.items()}\n",
    "\n",
    "original_dist = get_category_distribution(all_data)\n",
    "train_dist = get_category_distribution(train_data)\n",
    "val_dist = get_category_distribution(val_data)\n",
    "test_dist = get_category_distribution(test_data)\n",
    "\n",
    "print(f\"{'Category':<20} {'Original':<10} {'Train':<10} {'Val':<10} {'Test':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for category in sorted(original_dist.keys()):\n",
    "    print(f\"{category:<20} {original_dist[category]:.3f}     {train_dist[category]:.3f}     {val_dist[category]:.3f}     {test_dist[category]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
